{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is for optimizing the pipeline hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, cross_validate, KFold\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for storing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_dump(obj, fname):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def pickle_load(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def create_directory(dir_path=\"path/to/file\"):\n",
    "    \"\"\"\n",
    "    Wrapper function for the os.makedirs method.\n",
    "\n",
    "    Args:\n",
    "        dir_path (Any): The data to be pickled.\n",
    "\n",
    "    Returns:\n",
    "        Either the desired directory is created or a message stating it already exists.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(dir_path)\n",
    "    except FileExistsError:\n",
    "        print(f\"Path to {dir_path} already exists, yo.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameter grids for random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_setup_hp_grid(number_grid):\n",
    "    # Set up possible values of parameters to optimize over, follows autosklearn and assignment 2\n",
    "    params_dt = { 'classifier__criterion' : ['gini', 'entropy'],\n",
    "                'classifier__min_samples_leaf': np.linspace(1, 20, num=number_grid, dtype=int),\n",
    "                'classifier__min_samples_split': np.linspace(2, 20, num=number_grid, dtype=int) }\n",
    "\n",
    "    params_gb = {'classifier__learning_rate' : np.linspace(0.01, 1.0, num=number_grid), \n",
    "                'classifier__max_leaf_nodes' : np.linspace(3, 2047, num=number_grid, dtype=int), \n",
    "                'classifier__min_samples_leaf' : np.linspace(1, 200, num=number_grid, dtype=int),\n",
    "                'classifier__n_iter_no_change' : np.linspace(1, 20, num=number_grid, dtype=int),\n",
    "                'classifier__validation_fraction' : np.linspace(0.01, 0.4, num=number_grid)}\n",
    "\n",
    "    params_knn = {'classifier__n_neighbors' : np.linspace(1,100, num=number_grid, dtype=int),\n",
    "                'classifier__p' : [1,2],\n",
    "                'classifier__weights' : ['uniform', 'distance']}\n",
    "\n",
    "    params_log = {'classifier__C' : np.linspace(0.03125, 5, num=number_grid), ## Restricted the upper bound from 32768 to 5, to mimic autosklearn and preserve runtime\n",
    "                'classifier__tol' : np.linspace(0.00001, 0.1, num=number_grid),\n",
    "                'classifier__penalty' : ['l1', 'l2']}\n",
    "\n",
    "    params_rf = {'classifier__bootstrap' : [True, False],\n",
    "                'classifier__criterion' : ['gini', 'entropy'],\n",
    "                'classifier__min_samples_leaf': np.linspace(1, 20, num=number_grid, dtype=int), \n",
    "                'classifier__min_samples_split': np.linspace(2, 20, num=number_grid, dtype=int) }\n",
    "    \n",
    "    param_grids = [params_dt, params_gb, params_knn, params_log, params_rf]\n",
    "    return param_grids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameter grids that are compatible with BayesianSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skopt_setup_hp_grid():\n",
    "    # Set up possible values of parameters to optimize over, follows autosklearn and assignment 2\n",
    "    params_dt = { 'classifier__criterion' : Categorical(['gini', 'entropy']),\n",
    "                'classifier__min_samples_leaf': Integer(1,20),\n",
    "                'classifier__min_samples_split':  Integer(2,20) }\n",
    "\n",
    "    params_gb = {\n",
    "                'classifier__learning_rate' : Real(0.01, 1.0), \n",
    "                'classifier__max_leaf_nodes' :  Integer(3,2047), \n",
    "                'classifier__min_samples_leaf' :  Integer(1,200),\n",
    "                # 'classifier__n_iter_no_change' :  Integer(1,20), ## This throws errors and I don't know why >:(\n",
    "                'classifier__validation_fraction' : Real(0.01, 0.4)\n",
    "                }\n",
    "\n",
    "    params_knn = {'classifier__n_neighbors' :  Integer(1,100),\n",
    "                'classifier__p' : Categorical([1,2]),\n",
    "                'classifier__weights' : Categorical(['uniform', 'distance'])}\n",
    "\n",
    "    params_log = {'classifier__C' : Real(0.03125, 5.0), ## Restricted the upper bound from 32768 to 5, to mimic autosklearn and preserve runtime\n",
    "                'classifier__tol' : Real(0.00001, 0.1),\n",
    "                'classifier__penalty' : Categorical(['l1', 'l2'])}\n",
    "\n",
    "    params_rf = {'classifier__bootstrap' : Categorical([True, False]),\n",
    "                'classifier__criterion' : Categorical(['gini', 'entropy']),\n",
    "                'classifier__min_samples_leaf':  Integer(1,20), \n",
    "                'classifier__min_samples_split':  Integer(2,20) }\n",
    "    \n",
    "    param_grids = [params_dt, params_gb, params_knn, params_log, params_rf]\n",
    "    return param_grids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining your own \"steps\" in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelection(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, option='none'):\n",
    "        self.option = option\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        feature_selection = [\"alcohol\", \"density\", \"chlorides\", \"volatile acidity\",\n",
    "                              \"total sulfur dioxide\", \"fixed acidity\", \"pH\"]\n",
    "        \n",
    "        no_feature_selection = [\"alcohol\", \"density\", \"chlorides\", \"volatile acidity\",\n",
    "                                 \"total sulfur dioxide\", \"fixed acidity\", \"pH\",\n",
    "                                \"residual sugar\", \"sulphates\", \"citric acid\", \"free sulfur dioxide\"]\n",
    "        \n",
    "        new_X = pd.DataFrame.copy(X)\n",
    "        if self.option == 'none':\n",
    "            new_X = new_X[no_feature_selection]\n",
    "        elif self.option == 'yes':\n",
    "            new_X = new_X[feature_selection]\n",
    "        else:\n",
    "            print(\"Wrong option specified\")\n",
    "            return\n",
    "        \n",
    "        return new_X\n",
    "    \n",
    "class FeaturePreprocessing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, method = 'none'):\n",
    "        self.method = method\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.method == 'none':\n",
    "            return X\n",
    "\n",
    "        if self.method == 'pca':\n",
    "            transformer = PCA(random_state=5)\n",
    "            X = transformer.fit_transform(X)\n",
    "        elif self.method == 'fast_ica':\n",
    "            ## Had to modify this to allow for it to converge and lessen warnings\n",
    "            transformer = FastICA(random_state=5, whiten='unit-variance', max_iter=5000, tol=0.5)\n",
    "            X = transformer.fit_transform(X)\n",
    "        else:\n",
    "            print(\"Wrong option specified\")\n",
    "            return \n",
    "        return X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_pipeline(X, y, hpo_method):\n",
    "    classifier_names = ['decision_tree', 'gradient_boosting', 'k_nearest_neighbors',\n",
    "                        'logistic_regression', 'random_forest']\n",
    "    # Here are our classifiers\n",
    "    classifiers = [ DecisionTreeClassifier(random_state= 5),\n",
    "                    GradientBoostingClassifier(random_state= 5),\n",
    "                    KNeighborsClassifier(),\n",
    "                    LogisticRegression(random_state= 5, solver='liblinear'),\n",
    "                    RandomForestClassifier(random_state= 5)]\n",
    "\n",
    "    # Loop for each trial\n",
    "    all_data_accuracy = {}\n",
    "    all_data_hyperparams = {}\n",
    "\n",
    "    # hyperparameter ranges for classifiers:\n",
    "    params_grid_rs = sklearn_setup_hp_grid(30)\n",
    "    params_grid_bo = skopt_setup_hp_grid()\n",
    "\n",
    "\n",
    "    for j, classifier in enumerate(classifiers):\n",
    "        classifier_name = classifier_names[j]\n",
    "\n",
    "        # Setting the pipelines\n",
    "        steps = [('feature_selection', FeatureSelection()),\n",
    "                ('feature_preprocessor', FeaturePreprocessing() ),\n",
    "                ('classifier', classifier)]\n",
    "        pipeline = Pipeline(steps)\n",
    "\n",
    "        # Getting the pipeline hyperparameters:\n",
    "        if hpo_method == 'rs':\n",
    "            hyperparams = { 'feature_selection__option': ['none', 'yes'],\n",
    "                            'feature_preprocessor__method': ['pca', 'fast_ica', 'none']}\n",
    "            classifier_grid = params_grid_rs[j]\n",
    "            \n",
    "        if hpo_method == 'bo':\n",
    "            hyperparams = { 'feature_selection__option': Categorical(['none', 'yes']),\n",
    "                            'feature_preprocessor__method': Categorical(['pca', 'fast_ica', 'none'])}\n",
    "            classifier_grid = params_grid_bo[j]\n",
    "        \n",
    "        # Adding the classifier hyperparameters:\n",
    "        for key in classifier_grid:\n",
    "            hyperparams[key] = classifier_grid[key]\n",
    "\n",
    "        \n",
    "\n",
    "        inner_cv = KFold(n_splits=5, shuffle=True, random_state=5)\n",
    "        outer_cv = KFold(n_splits=5, shuffle=True, random_state=5)\n",
    "        if hpo_method == 'rs':\n",
    "            clf = RandomizedSearchCV(pipeline, hyperparams, cv=inner_cv, scoring= 'accuracy',\n",
    "                                     random_state= 5, n_iter = 30, n_jobs=3)\n",
    "        elif hpo_method == 'bo':\n",
    "            clf = BayesSearchCV(pipeline, hyperparams, cv=inner_cv, scoring= 'accuracy',\n",
    "                                random_state= 5, n_iter = 30, n_jobs = 3)\n",
    "        else:\n",
    "            print(\"Wrong hpo method given\")\n",
    "            return\n",
    "        \n",
    "        cv_result = cross_validate(clf, X=X, y=y, cv=outer_cv, return_estimator=True)\n",
    "\n",
    "        all_data_accuracy[classifier_name] =  cv_result['test_score']\n",
    "\n",
    "        hyperparams_runs = [cv_result['estimator'][i].best_params_ for i in range(5)]\n",
    "        all_data_hyperparams[classifier_name] = hyperparams_runs\n",
    "        print(f'classifier {classifier_name} is done')\n",
    "        \n",
    "    dir_path = './data_manual'\n",
    "    create_directory(dir_path)\n",
    "    pickle_dump(all_data_accuracy, f'{dir_path}/{hpo_method}_accuracy.pkl')\n",
    "    pickle_dump(all_data_hyperparams, f'{dir_path}/{hpo_method}_hyperparameters.pkl')  \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = \"winequality-white.csv\"\n",
    "table = pd.read_csv(fl, delimiter = \";\", header='infer')\n",
    "\n",
    "## yes, this gets all the features, it is redundant but it will fix itself in the pipeline\n",
    "no_feature_selection = [\"alcohol\", \"density\", \"chlorides\", \"volatile acidity\",\n",
    "                        \"total sulfur dioxide\", \"fixed acidity\", \"pH\",\n",
    "                        \"residual sugar\", \"sulphates\", \"citric acid\", \"free sulfur dioxide\"]\n",
    "X = table[no_feature_selection]\n",
    "y = table.quality\n",
    "\n",
    "# Need to do this:\n",
    "# pip install \"numpy < 1.24.0\"\n",
    "for hpo_method in ['bo', 'rs']:\n",
    "    sklearn_pipeline(X, y, hpo_method)\n",
    "    print(f'Done with method {hpo_method}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision_tree': array([0.61938776, 0.61734694, 0.58877551, 0.57609806, 0.58937692]),\n",
       " 'gradient_boosting': array([0.5744898 , 0.58979592, 0.63061224, 0.61389173, 0.52298264]),\n",
       " 'k_nearest_neighbors': array([0.65612245, 0.63163265, 0.63877551, 0.62206333, 0.64249234]),\n",
       " 'logistic_regression': array([0.5622449 , 0.52142857, 0.54693878, 0.51787538, 0.52808989]),\n",
       " 'random_forest': array([0.69591837, 0.69081633, 0.70816327, 0.66496425, 0.67313585])}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_load('./data_manual/bo_accuracy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__criterion': 'entropy', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__criterion': 'gini', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__criterion': 'gini', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__criterion': 'gini', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__criterion': 'gini', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'yes'}\n",
      "{'classifier__learning_rate': 0.7479753502026856, 'classifier__max_leaf_nodes': 1680, 'classifier__min_samples_leaf': 172, 'classifier__validation_fraction': 0.1632394200503056, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__learning_rate': 0.17062917981880404, 'classifier__max_leaf_nodes': 2043, 'classifier__min_samples_leaf': 33, 'classifier__validation_fraction': 0.4, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__learning_rate': 0.4618247519485242, 'classifier__max_leaf_nodes': 2047, 'classifier__min_samples_leaf': 1, 'classifier__validation_fraction': 0.4, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__learning_rate': 0.3315561627279408, 'classifier__max_leaf_nodes': 2047, 'classifier__min_samples_leaf': 1, 'classifier__validation_fraction': 0.032048484730744564, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__learning_rate': 0.7474443145714168, 'classifier__max_leaf_nodes': 1223, 'classifier__min_samples_leaf': 174, 'classifier__validation_fraction': 0.20959485725118232, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__n_neighbors': 66, 'classifier__p': 1, 'classifier__weights': 'distance', 'feature_preprocessor__method': 'none', 'feature_selection__option': 'yes'}\n",
      "{'classifier__n_neighbors': 61, 'classifier__p': 1, 'classifier__weights': 'distance', 'feature_preprocessor__method': 'none', 'feature_selection__option': 'yes'}\n",
      "{'classifier__n_neighbors': 83, 'classifier__p': 1, 'classifier__weights': 'distance', 'feature_preprocessor__method': 'none', 'feature_selection__option': 'yes'}\n",
      "{'classifier__n_neighbors': 85, 'classifier__p': 1, 'classifier__weights': 'distance', 'feature_preprocessor__method': 'none', 'feature_selection__option': 'yes'}\n",
      "{'classifier__n_neighbors': 59, 'classifier__p': 1, 'classifier__weights': 'distance', 'feature_preprocessor__method': 'none', 'feature_selection__option': 'yes'}\n",
      "{'classifier__C': 5.0, 'classifier__penalty': 'l1', 'classifier__tol': 1e-05, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__C': 4.8745586414989415, 'classifier__penalty': 'l1', 'classifier__tol': 0.07089542207119612, 'feature_preprocessor__method': 'pca', 'feature_selection__option': 'none'}\n",
      "{'classifier__C': 5.0, 'classifier__penalty': 'l1', 'classifier__tol': 1e-05, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__C': 5.0, 'classifier__penalty': 'l1', 'classifier__tol': 1e-05, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__C': 5.0, 'classifier__penalty': 'l1', 'classifier__tol': 1e-05, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__bootstrap': True, 'classifier__criterion': 'entropy', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__bootstrap': True, 'classifier__criterion': 'gini', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__bootstrap': False, 'classifier__criterion': 'entropy', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__bootstrap': False, 'classifier__criterion': 'entropy', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 11, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n",
      "{'classifier__bootstrap': False, 'classifier__criterion': 'entropy', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 9, 'feature_preprocessor__method': 'none', 'feature_selection__option': 'none'}\n"
     ]
    }
   ],
   "source": [
    "temp = pickle_load('./data_manual/bo_hyperparameters.pkl')\n",
    "for ordered_dict in temp.values():\n",
    "    for ord_dict in ordered_dict:\n",
    "        print(dict(ord_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision_tree': array([0.55204082, 0.51326531, 0.53367347, 0.52502554, 0.52093973]),\n",
       " 'gradient_boosting': array([0.58163265, 0.55612245, 0.52755102, 0.52911134, 0.53728294]),\n",
       " 'k_nearest_neighbors': array([0.6255102 , 0.61020408, 0.61938776, 0.57303371, 0.59652707]),\n",
       " 'logistic_regression': array([0.53571429, 0.49489796, 0.52653061, 0.49336057, 0.48927477]),\n",
       " 'random_forest': array([0.61326531, 0.6       , 0.61734694, 0.59141982, 0.57201226])}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_load('./data_manual/rs/accuracy.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
